{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Group GEMM\nThis group gemm kernel launches a fixed number of CTA to compute a group\nof gemms. The scheduling is static and we do it on device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2023 - 2025 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom typing import Optional\nimport torch\n\nimport triton\nimport triton.language as tl\n\nDEVICE = triton.runtime.driver.active.get_active_torch_device()\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef supports_tma():\n    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n\n\ndef num_sms():\n    if is_cuda():\n        return torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    return 148\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\n            'BLOCK_SIZE_M': 128,\n            'BLOCK_SIZE_N': 128,\n            'BLOCK_SIZE_K': 32,\n            'NUM_SM': 84,\n        }),\n        triton.Config({\n            'BLOCK_SIZE_M': 128,\n            'BLOCK_SIZE_N': 128,\n            'BLOCK_SIZE_K': 32,\n            'NUM_SM': 128,\n        }),\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 64,\n            'BLOCK_SIZE_K': 32,\n            'NUM_SM': 84,\n        }),\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 64,\n            'BLOCK_SIZE_K': 32,\n            'NUM_SM': 128,\n        }),\n        triton.Config({\n            'BLOCK_SIZE_M': 128,\n            'BLOCK_SIZE_N': 128,\n            'BLOCK_SIZE_K': 64,\n            'NUM_SM': num_sms(),\n        }),\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 128,\n            'BLOCK_SIZE_K': 64,\n            'NUM_SM': num_sms(),\n        }),\n    ],\n    key=['group_size'],\n)\n@triton.jit\ndef grouped_matmul_kernel(\n    # device tensor of matrices pointers\n    group_a_ptrs,\n    group_b_ptrs,\n    group_c_ptrs,\n    # device tensor of gemm sizes. its shape is [group_size, 3]\n    # dim 0 is group_size, dim 1 is the values of <M, N, K> of each gemm\n    group_gemm_sizes,\n    # device tensor of leading dimension sizes. its shape is [group_size, 3]\n    # dim 0 is group_size, dim 1 is the values of <lda, ldb, ldc> of each gemm\n    g_lds,\n    # number of gemms\n    group_size,\n    # number of virtual SM\n    NUM_SM: tl.constexpr,\n    # tile sizes\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    tile_idx = tl.program_id(0)\n    last_problem_end = 0\n    for g in range(group_size):\n        # get the gemm size of the current problem\n        gm = tl.load(group_gemm_sizes + g * 3)\n        gn = tl.load(group_gemm_sizes + g * 3 + 1)\n        gk = tl.load(group_gemm_sizes + g * 3 + 2)\n        num_m_tiles = tl.cdiv(gm, BLOCK_SIZE_M)\n        num_n_tiles = tl.cdiv(gn, BLOCK_SIZE_N)\n        num_tiles = num_m_tiles * num_n_tiles\n        # iterate through the tiles in the current gemm problem\n        while (tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles):\n            # pick up a tile from the current gemm problem\n            k = gk\n            lda = tl.load(g_lds + g * 3)\n            ldb = tl.load(g_lds + g * 3 + 1)\n            ldc = tl.load(g_lds + g * 3 + 2)\n            a_ptr = tl.load(group_a_ptrs + g).to(tl.pointer_type(tl.float16))\n            b_ptr = tl.load(group_b_ptrs + g).to(tl.pointer_type(tl.float16))\n            c_ptr = tl.load(group_c_ptrs + g).to(tl.pointer_type(tl.float16))\n            # figure out tile coordinates\n            tile_idx_in_gemm = tile_idx - last_problem_end\n            tile_m_idx = tile_idx_in_gemm // num_n_tiles\n            tile_n_idx = tile_idx_in_gemm % num_n_tiles\n\n            # do regular gemm here\n            offs_am = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            offs_k = tl.arange(0, BLOCK_SIZE_K)\n            a_ptrs = a_ptr + offs_am[:, None] * lda + offs_k[None, :]\n            b_ptrs = b_ptr + offs_k[:, None] * ldb + offs_bn[None, :]\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n            for kk in range(0, tl.cdiv(k, BLOCK_SIZE_K)):\n                # hint to Triton compiler to do proper loop pipelining\n                tl.multiple_of(a_ptrs, [16, 16])\n                tl.multiple_of(b_ptrs, [16, 16])\n                # assume full tile for now\n                a = tl.load(a_ptrs)\n                b = tl.load(b_ptrs)\n                accumulator += tl.dot(a, b)\n                a_ptrs += BLOCK_SIZE_K\n                b_ptrs += BLOCK_SIZE_K * ldb\n            c = accumulator.to(tl.float16)\n\n            offs_cm = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + ldc * offs_cm[:, None] + offs_cn[None, :]\n\n            # assumes full tile for now\n            tl.store(c_ptrs, c)\n\n            # go to the next tile by advancing NUM_SM\n            tile_idx += NUM_SM\n\n        # get ready to go to the next gemm problem\n        last_problem_end = last_problem_end + num_tiles\n\n\ndef group_gemm_fn(group_A, group_B):\n    assert len(group_A) == len(group_B)\n    group_size = len(group_A)\n\n    A_addrs = []\n    B_addrs = []\n    C_addrs = []\n    g_sizes = []\n    g_lds = []\n    group_C = []\n    for i in range(group_size):\n        A = group_A[i]\n        B = group_B[i]\n        assert A.shape[1] == B.shape[0]\n        M, K = A.shape\n        K, N = B.shape\n        C = torch.empty((M, N), device=DEVICE, dtype=A.dtype)\n        group_C.append(C)\n        A_addrs.append(A.data_ptr())\n        B_addrs.append(B.data_ptr())\n        C_addrs.append(C.data_ptr())\n        g_sizes += [M, N, K]\n        g_lds += [A.stride(0), B.stride(0), C.stride(0)]\n\n    # note these are device tensors\n    d_a_ptrs = torch.tensor(A_addrs, device=DEVICE)\n    d_b_ptrs = torch.tensor(B_addrs, device=DEVICE)\n    d_c_ptrs = torch.tensor(C_addrs, device=DEVICE)\n    d_g_sizes = torch.tensor(g_sizes, dtype=torch.int32, device=DEVICE)\n    d_g_lds = torch.tensor(g_lds, dtype=torch.int32, device=DEVICE)\n    # we use a fixed number of CTA, and it's auto-tunable\n    grid = lambda META: (META['NUM_SM'], )\n    grouped_matmul_kernel[grid](\n        d_a_ptrs,\n        d_b_ptrs,\n        d_c_ptrs,\n        d_g_sizes,\n        d_g_lds,\n        group_size,\n    )\n\n    return group_C\n\n\ntma_configs = [\n    triton.Config({'BLOCK_SIZE_M': BM, 'BLOCK_SIZE_N': BN, 'BLOCK_SIZE_K' : BK}, num_stages=s, num_warps=w) \\\n    for BM in [128]\\\n    for BN in [128, 256]\\\n    for BK in [64, 128]\\\n    for s in ([3, 4])\\\n    for w in [4, 8]\\\n]\n\n\n@triton.autotune(\n    tma_configs,\n    key=['group_a_ptrs', 'group_b_ptrs', 'gropup_c_ptrs', 'group_size'],\n)\n@triton.jit\ndef grouped_matmul_tma_kernel(\n    # device tensor of matrices pointers\n    group_a_ptrs,\n    group_b_ptrs,\n    group_c_ptrs,\n    # device tensor of gemm sizes. its shape is [group_size, 3]\n    # dim 0 is group_size, dim 1 is the values of <M, N, K> of each gemm\n    group_gemm_sizes,\n    # device tensor of leading dimension sizes. its shape is [group_size, 3]\n    # dim 0 is group_size, dim 1 is the values of <lda, ldb, ldc> of each gemm\n    g_lds,\n    # number of gemms\n    group_size,\n    # number of virtual SM\n    NUM_SM: tl.constexpr,\n    # tile sizes\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    # is the output FP8 or FP16\n    FP8: tl.constexpr,\n):\n    dtype = tl.float8e4nv if FP8 else tl.float16\n    tile_idx = tl.program_id(0)\n    last_problem_end = 0\n    for g in range(group_size):\n        # get the gemm size of the current problem\n        gm = tl.load(group_gemm_sizes + g * 3)\n        gn = tl.load(group_gemm_sizes + g * 3 + 1)\n        gk = tl.load(group_gemm_sizes + g * 3 + 2)\n        num_m_tiles = tl.cdiv(gm, BLOCK_SIZE_M)\n        num_n_tiles = tl.cdiv(gn, BLOCK_SIZE_N)\n        num_tiles = num_m_tiles * num_n_tiles\n        if tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:\n            # pick up a tile from the current gemm problem\n            lda = tl.load(g_lds + g * 3)\n            ldb = tl.load(g_lds + g * 3 + 1)\n            ldc = tl.load(g_lds + g * 3 + 2)\n\n            a_ptr = tl.load(group_a_ptrs + g).to(tl.pointer_type(dtype))\n            b_ptr = tl.load(group_b_ptrs + g).to(tl.pointer_type(dtype))\n            c_ptr = tl.load(group_c_ptrs + g).to(tl.pointer_type(dtype))\n\n            a_desc = tl.make_tensor_descriptor(\n                a_ptr,\n                shape=[gm, gk],\n                strides=[lda, 1],\n                block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],\n            )\n\n            b_desc = tl.make_tensor_descriptor(\n                b_ptr,\n                shape=[gn, gk],\n                strides=[ldb, 1],\n                block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],\n            )\n            c_desc = tl.make_tensor_descriptor(\n                c_ptr,\n                shape=[gm, gn],\n                strides=[ldc, 1],\n                block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N],\n            )\n\n            # iterate through the tiles in the current gemm problem\n            while (tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles):\n                k = gk\n                # figure out tile coordinates\n                tile_idx_in_gemm = tile_idx - last_problem_end\n                tile_m_idx = tile_idx_in_gemm // num_n_tiles\n                tile_n_idx = tile_idx_in_gemm % num_n_tiles\n\n                # do regular gemm here\n                offs_am = tile_m_idx * BLOCK_SIZE_M\n                offs_bn = tile_n_idx * BLOCK_SIZE_N\n\n                accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n                for kk in range(0, tl.cdiv(k, BLOCK_SIZE_K)):\n                    a = a_desc.load([offs_am, kk * BLOCK_SIZE_K])\n                    b = b_desc.load([offs_bn, kk * BLOCK_SIZE_K])\n                    accumulator += tl.dot(a, b.T)\n\n                offs_cm = tile_m_idx * BLOCK_SIZE_M\n                offs_cn = tile_n_idx * BLOCK_SIZE_N\n\n                c = accumulator.to(dtype)\n                c_desc.store([offs_cm, offs_cn], c)\n\n                # go to the next tile by advancing NUM_SM\n                tile_idx += NUM_SM\n\n        # get ready to go to the next gemm problem\n        last_problem_end = last_problem_end + num_tiles\n\n\ndef group_gemm_tma_fn(group_A, group_B):\n\n    assert supports_tma()\n\n    assert len(group_A) == len(group_B)\n    group_size = len(group_A)\n\n    A_addrs = []\n    B_addrs = []\n    C_addrs = []\n    g_sizes = []\n    g_lds = []\n    group_C = []\n    for i in range(group_size):\n        A = group_A[i]\n        B = group_B[i]\n        assert A.shape[1] == B.shape[1]\n        M, K = A.shape\n        N, K = B.shape\n        C = torch.empty((M, N), device=DEVICE, dtype=A.dtype)\n        group_C.append(C)\n        A_addrs.append(A.data_ptr())\n        B_addrs.append(B.data_ptr())\n        C_addrs.append(C.data_ptr())\n        g_sizes += [M, N, K]\n        g_lds += [A.stride(0), B.stride(0), C.stride(0)]\n    # note these are device tensors\n    d_a_ptrs = torch.tensor(A_addrs, device=DEVICE)\n    d_b_ptrs = torch.tensor(B_addrs, device=DEVICE)\n    d_c_ptrs = torch.tensor(C_addrs, device=DEVICE)\n    d_g_sizes = torch.tensor(g_sizes, dtype=torch.int32, device=DEVICE)\n    d_g_lds = torch.tensor(g_lds, dtype=torch.int32, device=DEVICE)\n\n    # we use a fixed number of CTA, and it's auto-tunable\n\n    # TMA descriptors require a global memory allocation\n    def alloc_fn(size: int, alignment: int, stream: Optional[int]):\n        return torch.empty(size, device=\"cuda\", dtype=torch.int8)\n\n    triton.set_allocator(alloc_fn)\n\n    grid = lambda META: (META['NUM_SM'], )\n    grouped_matmul_tma_kernel[grid](d_a_ptrs, d_b_ptrs, d_c_ptrs, d_g_sizes, d_g_lds, group_size,\n                                    FP8=torch.float8_e4m3fn == group_A[0].dtype, NUM_SM=num_sms())\n    return group_C\n\n\ngroup_m = [1024, 512, 256, 128]\ngroup_n = [1024, 512, 256, 128]\ngroup_k = [1024, 512, 256, 128]\ngroup_A = []\ngroup_B = []\ngroup_B_T = []\nassert len(group_m) == len(group_n)\nassert len(group_n) == len(group_k)\ngroup_size = len(group_m)\nfor i in range(group_size):\n    M = group_m[i]\n    N = group_n[i]\n    K = group_k[i]\n    A = torch.rand((M, K), device=DEVICE, dtype=torch.float16)\n    B = torch.rand((K, N), device=DEVICE, dtype=torch.float16)\n    B_T = B.T.contiguous()\n    group_A.append(A)\n    group_B.append(B)\n    group_B_T.append(B_T)\n\ntri_out = group_gemm_fn(group_A, group_B)\nref_out = [torch.matmul(a, b) for a, b in zip(group_A, group_B)]\nfor i in range(group_size):\n    assert torch.allclose(ref_out[i], tri_out[i], atol=1e-2, rtol=1e-2)\n\nif supports_tma():\n    tri_tma_out = group_gemm_tma_fn(group_A, group_B_T)\n    for i in range(group_size):\n        assert torch.allclose(ref_out[i], tri_tma_out[i], atol=1e-2, rtol=1e-2)\n\n\n# only launch the kernel, no tensor preparation here to remove all overhead\ndef triton_perf_fn(a_ptrs, b_ptrs, c_ptrs, sizes, lds, group_size):\n    grid = lambda META: (META['NUM_SM'], )\n    grouped_matmul_kernel[grid](\n        a_ptrs,\n        b_ptrs,\n        c_ptrs,\n        sizes,\n        lds,\n        group_size,\n    )\n\n\ndef triton_tma_perf_fn(a_ptrs, b_ptrs, c_ptrs, sizes, lds, group_size, dtype):\n    grid = lambda META: (META['NUM_SM'], )\n    grouped_matmul_tma_kernel[grid](a_ptrs, b_ptrs, c_ptrs, sizes, lds, group_size, FP8=torch.float8_e4m3fn == dtype,\n                                    NUM_SM=num_sms())\n\n\ndef torch_perf_fn(group_A, group_B):\n    for a, b in zip(group_A, group_B):\n        torch.matmul(a, b)\n\n\n@triton.testing.perf_report(\n    triton.testing.Benchmark(\n        # argument names to use as an x-axis for the plot\n        x_names=['N'],\n        x_vals=[2**i for i in range(7, 11)],  # different possible values for `x_name`\n        line_arg='provider',\n        # argument name whose value corresponds to a different line in the plot\n        # possible values for `line_arg``\n        line_vals=['cublas', 'triton'] + (['triton-tma'] if supports_tma() else []),\n        # label name for the lines\n        line_names=[\"cuBLAS\", \"Triton\"] + (['Triton + TMA'] if supports_tma() else []),\n        # line styles\n        styles=[('green', '-'), ('blue', '-')] + ([('red', '-')] if supports_tma() else []),\n        ylabel=\"runtime(ms)\",  # label name for the y-axis\n        plot_name=\"group-gemm-performance\",\n        # name for the plot. Used also as a file name for saving the plot.\n        args={},\n    ))\ndef benchmark_square_matrices(N, provider):\n    group_size = 4\n    group_A = []\n    group_B = []\n    group_B_T = []\n    A_addrs = []\n    B_addrs = []\n    B_T_addrs = []\n    C_addrs = []\n    g_sizes = []\n    g_lds = []\n    group_C = []\n    for i in range(group_size):\n        A = torch.rand((N, N), device=DEVICE, dtype=torch.float16)\n        B = torch.rand((N, N), device=DEVICE, dtype=torch.float16)\n        C = torch.empty((N, N), device=DEVICE, dtype=torch.float16)\n        B_T = B.T.contiguous()\n        group_A.append(A)\n        group_B.append(B)\n        group_B_T.append(B_T)\n        group_C.append(C)\n        A_addrs.append(A.data_ptr())\n        B_addrs.append(B.data_ptr())\n        B_T_addrs.append(B_T.data_ptr())\n        C_addrs.append(C.data_ptr())\n        g_sizes += [N, N, N]\n        g_lds += [N, N, N]\n\n    d_a_ptrs = torch.tensor(A_addrs, device=DEVICE)\n    d_b_ptrs = torch.tensor(B_addrs, device=DEVICE)\n    d_b_t_ptrs = torch.tensor(B_T_addrs, device=DEVICE)\n    d_c_ptrs = torch.tensor(C_addrs, device=DEVICE)\n    d_g_sizes = torch.tensor(g_sizes, dtype=torch.int32, device=DEVICE)\n    d_g_lds = torch.tensor(g_lds, dtype=torch.int32, device=DEVICE)\n\n    quantiles = [0.5, 0.2, 0.8]\n    if provider == 'cublas':\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch_perf_fn(group_A, group_B), quantiles=quantiles)\n    if provider == 'triton':\n        ms, min_ms, max_ms = triton.testing.do_bench(\n            lambda: triton_perf_fn(d_a_ptrs, d_b_ptrs, d_c_ptrs, d_g_sizes, d_g_lds, group_size), quantiles=quantiles)\n    if provider == 'triton-tma':\n        ms, min_ms, max_ms = triton.testing.do_bench(\n            lambda: triton_tma_perf_fn(d_a_ptrs, d_b_t_ptrs, d_c_ptrs, d_g_sizes, d_g_lds, group_size, dtype=torch.\n                                       float16), quantiles=quantiles)\n    return ms, max_ms, min_ms\n\n\n@triton.testing.perf_report(\n    triton.testing.Benchmark(\n        # argument names to use as an x-axis for the plot\n        x_names=['M'],\n        x_vals=[2**i for i in range(7, 11)],  # different possible values for `x_name`\n        line_arg='provider',\n        # argument name whose value corresponds to a different line in the plot\n        # possible values for `line_arg``\n        line_vals=['cublas', 'triton'] + (['triton-tma'] if supports_tma() else []),\n        # label name for the lines\n        line_names=[\"cuBLAS\", \"Triton\"] + (['Triton + TMA'] if supports_tma() else []),\n        # line styles\n        styles=[('green', '-'), ('blue', '-')] + ([('red', '-')] if supports_tma() else []),\n        ylabel=\"runtime(ms)\",  # label name for the y-axis\n        plot_name=\"group-gemm-performance-m-8192-k-8192\",\n        # name for the plot. Used also as a file name for saving the plot.\n        args={},\n    ))\ndef benchmark_batches(M, provider):\n    N = 8192\n    K = 8192\n    group_size = 4\n    group_A = []\n    group_B = []\n    group_B_T = []\n    A_addrs = []\n    B_addrs = []\n    B_T_addrs = []\n    C_addrs = []\n    g_sizes = []\n    g_lds = []\n    g_T_lds = []\n    group_C = []\n    for i in range(group_size):\n        A = torch.rand((M, K), device=DEVICE, dtype=torch.float16)\n        B = torch.rand((K, N), device=DEVICE, dtype=torch.float16)\n        C = torch.empty((M, N), device=DEVICE, dtype=torch.float16)\n        B_T = B.T.contiguous()\n        group_A.append(A)\n        group_B.append(B)\n        group_B_T.append(B_T)\n        group_C.append(C)\n        A_addrs.append(A.data_ptr())\n        B_addrs.append(B.data_ptr())\n        B_T_addrs.append(B_T.data_ptr())\n        C_addrs.append(C.data_ptr())\n        g_sizes += [M, N, K]\n        g_lds += [A.stride(0), B.stride(0), C.stride(0)]\n        g_T_lds += [A.stride(0), B_T.stride(0), C.stride(0)]\n\n    d_a_ptrs = torch.tensor(A_addrs, device=DEVICE)\n    d_b_ptrs = torch.tensor(B_addrs, device=DEVICE)\n    d_b_t_ptrs = torch.tensor(B_T_addrs, device=DEVICE)\n    d_c_ptrs = torch.tensor(C_addrs, device=DEVICE)\n    d_g_sizes = torch.tensor(g_sizes, dtype=torch.int32, device=DEVICE)\n    d_g_lds = torch.tensor(g_lds, dtype=torch.int32, device=DEVICE)\n    d_g_t_lds = torch.tensor(g_T_lds, dtype=torch.int32, device=DEVICE)\n\n    quantiles = [0.5, 0.2, 0.8]\n    if provider == 'cublas':\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch_perf_fn(group_A, group_B), quantiles=quantiles)\n    if provider == 'triton':\n        ms, min_ms, max_ms = triton.testing.do_bench(\n            lambda: triton_perf_fn(d_a_ptrs, d_b_ptrs, d_c_ptrs, d_g_sizes, d_g_lds, group_size), quantiles=quantiles)\n    if provider == 'triton-tma':\n        ms, min_ms, max_ms = triton.testing.do_bench(\n            lambda: triton_tma_perf_fn(d_a_ptrs, d_b_t_ptrs, d_c_ptrs, d_g_sizes, d_g_t_lds, group_size, dtype=torch.\n                                       float16), quantiles=quantiles)\n    return ms, max_ms, min_ms\n\n\nbenchmark_square_matrices.run(show_plots=True, print_data=True)\nbenchmark_batches.run(show_plots=True, print_data=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}