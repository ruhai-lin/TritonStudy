{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Block Scaled Matrix Multiplication\nThis tutorial demonstrates a Triton implementation of block scaled matrix multiplication\nwhich is generic over FP4 and FP8 formats. The formats supported in the tutorial are the OCP microscaling\nformats, including mxfp4 and mxfp8, as well as NVIDIA's nvfp4 format. These matrix multiplications\nare accelerated by fifth generation tensor core instructions on CUDA devices with compute capability 10.\n\nUsers can run the tutorial with each of the supported formats by passing the `--format`\nargument and can benchmark the performance of each by specifying matrix dimensions\nand iteration steps.\n\n```bash\n# FP4\npython 10-block-scaled-matmul.py --format nvfp4\npython 10-block-scaled-matmul.py --format mxfp4 --K_range 512 8192 --bench\n\n# FP8\npython 10-block-scaled-matmul.py --format mxfp8 --K_range 8192 16384 --K_step 2048 --bench\n```\nFuture updates to this tutorial which support mixed precision block scaled matmul are planned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background\n\nCUDA devices that support PTX 8.7 and later can utlize block scaled matrix multiply\ninstructions. In order for low latency access to these scale factors in the fast\ninner loop over tensor core MMAs, it is important to ensure that the blocked\nscale factors are stored in a contiguous memory layout according to their access\npattern.\n\nThe block scaled matmul tensor core instructions compute the following product:\n\n    C = (A * scale_a) @ (B * scale_b)\n\nwhere scale_a and scale_b are the blocked scale factors for the A and B matrices.\nUnder block scaled matmul, each scale factor is broadcast and multiplied across a\nvector of elements from the A and B matrices, usually along their respective K axes.\nThe number of elements of A and B over which each scale factor is broadcast is herein\nrefered to as the vector size (VEC_SIZE).\n\nIn a linear row-major layout, the scale factors would take the shape\n\n    (M, K // VEC_SIZE) and (N, K // VEC_SIZE)   [1]\n\nin global memory. However, to avoid non-contiguous memory access, it is beneficial to\ninstead store the scale factors in a packed block layout. For the LHS matrix this layout\nis given by\n\n    (M // 32 // 4, K // VEC_SIZE // 4, 32, 4, 4)   [2].\n\nIn this way, each tensor core MMA in the fast inner loop over K blocks can achieve contiguous\naccess of a block of 128 rows of scale factors along the M axis, for each BLOCK_M x BLOCK_K\nsubtile of the matrix A.\n\nIn order to conform with Triton's language semantics for dot_scaled, the scale factors\nare prepared in the above 5D layout [2], but are then logically transposed and reshaped into\nthe 2D layout [1] expected by tl.dot_scaled.\n\nFor more detailed information on the scale factor layout, see\n 1. https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-a-layout-1x\n 2. https://docs.nvidia.com/cuda/cublas/#d-block-scaling-factors-layout\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\n\nimport torch\nimport triton\nimport triton.language as tl\nimport triton.profiler as proton\nfrom triton.tools.tensor_descriptor import TensorDescriptor\nfrom triton.tools.mxfp import MXFP4Tensor, MXScaleTensor\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef supports_block_scaling():\n    return is_cuda() and torch.cuda.get_device_capability()[0] == 10\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    kernel_name = kernel.name\n    if \"ELEM_PER_BYTE_A\" and \"ELEM_PER_BYTE_B\" and \"VEC_SIZE\" in args:\n        if args[\"ELEM_PER_BYTE_A\"] == 1 and args[\"ELEM_PER_BYTE_B\"] == 1:\n            kernel_name += \"_mxfp8\"\n        elif args[\"ELEM_PER_BYTE_A\"] == 1 and args[\"ELEM_PER_BYTE_B\"] == 2:\n            kernel_name += \"_mixed\"\n        elif args[\"ELEM_PER_BYTE_A\"] == 2 and args[\"ELEM_PER_BYTE_B\"] == 2:\n            if args[\"VEC_SIZE\"] == 16:\n                kernel_name += \"_nvfp4\"\n            elif args[\"VEC_SIZE\"] == 32:\n                kernel_name += \"_mxfp4\"\n    ret[\"name\"] = f\"{kernel_name} [M={M}, N={N}, K={K}]\"\n    ret[\"flops\"] = 2.0 * M * N * K\n    return ret\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef block_scaled_matmul_kernel(  #\n        a_desc,  #\n        a_scale_desc,  #\n        b_desc,  #\n        b_scale_desc,  #\n        c_desc,  #\n        M: tl.constexpr,  #\n        N: tl.constexpr,  #\n        K: tl.constexpr,  #\n        output_type: tl.constexpr,  #\n        ELEM_PER_BYTE_A: tl.constexpr,  #\n        ELEM_PER_BYTE_B: tl.constexpr,  #\n        VEC_SIZE: tl.constexpr,  #\n        BLOCK_M: tl.constexpr,  #\n        BLOCK_N: tl.constexpr,  #\n        BLOCK_K: tl.constexpr,  #\n        rep_m: tl.constexpr,  #\n        rep_n: tl.constexpr,  #\n        rep_k: tl.constexpr,  #\n        NUM_STAGES: tl.constexpr,  #\n):  #\n    if output_type == 0:\n        output_dtype = tl.float32\n    elif output_type == 1:\n        output_dtype = tl.float16\n    elif output_type == 2:\n        output_dtype = tl.float8e4nv\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n    offs_am = pid_m * BLOCK_M\n    offs_bn = pid_n * BLOCK_N\n    offs_k_a = 0\n    offs_k_b = 0\n    offs_scale_m = pid_m * rep_m\n    offs_scale_n = pid_n * rep_n\n    offs_scale_k = 0\n\n    MIXED_PREC: tl.constexpr = ELEM_PER_BYTE_A == 1 and ELEM_PER_BYTE_B == 2\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in tl.range(0, tl.cdiv(K, BLOCK_K), num_stages=NUM_STAGES):\n        a = a_desc.load([offs_am, offs_k_a])\n        b = b_desc.load([offs_bn, offs_k_b])\n        scale_a = a_scale_desc.load([0, offs_scale_m, offs_scale_k, 0, 0])\n        scale_b = b_scale_desc.load([0, offs_scale_n, offs_scale_k, 0, 0])\n\n        scale_a = scale_a.reshape(rep_m, rep_k, 32, 4, 4).trans(0, 3, 2, 1, 4).reshape(BLOCK_M, BLOCK_K // VEC_SIZE)\n        scale_b = scale_b.reshape(rep_n, rep_k, 32, 4, 4).trans(0, 3, 2, 1, 4).reshape(BLOCK_N, BLOCK_K // VEC_SIZE)\n\n        if MIXED_PREC:\n            accumulator = tl.dot_scaled(a, scale_a, \"e4m3\", b.T, scale_b, \"e2m1\", accumulator)\n        elif ELEM_PER_BYTE_A == 2 and ELEM_PER_BYTE_B == 2:\n            accumulator = tl.dot_scaled(a, scale_a, \"e2m1\", b.T, scale_b, \"e2m1\", accumulator)\n        else:\n            accumulator = tl.dot_scaled(a, scale_a, \"e4m3\", b.T, scale_b, \"e4m3\", accumulator)\n\n        offs_k_a += BLOCK_K // ELEM_PER_BYTE_A\n        offs_k_b += BLOCK_K // ELEM_PER_BYTE_B\n        offs_scale_k += rep_k\n\n    c_desc.store([offs_am, offs_bn], accumulator.to(output_dtype))\n\n\ndef block_scaled_matmul(a_desc, a_scale_desc, b_desc, b_scale_desc, dtype_dst, M, N, K, rep_m, rep_n, rep_k, configs):\n    output = torch.empty((M, N), dtype=dtype_dst, device=\"cuda\")\n    if dtype_dst == torch.float32:\n        dtype_dst = 0\n    elif dtype_dst == torch.float16:\n        dtype_dst = 1\n    elif dtype_dst == torch.float8_e4m3fn:\n        dtype_dst = 2\n    else:\n        raise ValueError(f\"Unsupported dtype: {dtype_dst}\")\n\n    BLOCK_M = configs[\"BLOCK_SIZE_M\"]\n    BLOCK_N = configs[\"BLOCK_SIZE_N\"]\n    c_desc = TensorDescriptor.from_tensor(output, [BLOCK_M, BLOCK_N])\n\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), 1)\n    block_scaled_matmul_kernel[grid](\n        a_desc,\n        a_scale_desc,\n        b_desc,\n        b_scale_desc,\n        c_desc,\n        M,\n        N,\n        K,\n        dtype_dst,\n        configs[\"ELEM_PER_BYTE_A\"],\n        configs[\"ELEM_PER_BYTE_B\"],\n        configs[\"VEC_SIZE\"],\n        configs[\"BLOCK_SIZE_M\"],\n        configs[\"BLOCK_SIZE_N\"],\n        configs[\"BLOCK_SIZE_K\"],\n        rep_m,\n        rep_n,\n        rep_k,\n        configs[\"num_stages\"],\n    )\n    return output\n\n\ndef initialize_block_scaled(M, N, K, block_scale_type=\"nvfp4\", compute_reference=False):\n    BLOCK_M = 128\n    BLOCK_N = 256\n    BLOCK_K = 256 if \"fp4\" in block_scale_type else 128\n    VEC_SIZE = 16 if block_scale_type == \"nvfp4\" else 32\n    assert block_scale_type in [\"nvfp4\", \"mxfp4\", \"mxfp8\", \"mixed\"], f\"Invalid block scale type: {block_scale_type}\"\n    ELEM_PER_BYTE_A = 2 if \"fp4\" in block_scale_type else 1\n    ELEM_PER_BYTE_B = 1 if block_scale_type == \"mxfp8\" else 2\n\n    device = \"cuda\"\n    a_ref = MXFP4Tensor(size=(M, K), device=device).random()\n    # Similar to Hopper's wgmma symmetric fp8 instruction, the RHS is expected\n    # to be in col-major layout for Blackwell's tcgen05.mma when using fp4 operands.\n    # To conform to the expected semantics of tl.dot_scaled, (M, K) x (K, N),\n    # the data is generated in col-major layout, packed along K for fp4, and then\n    # logically transposed. Note that if one operand is of fp8 precision, unlike Hopper,\n    # Blackwell supports both row-major and col-major layouts for the RHS matrix.\n    # For the mixed-precision case, the fp4 RHS can be either in row or col-major layout.\n    # But for performance reason, it is recommended to use col-major layout. If TMA is used\n    # for the fp4 RHS operand load in mixed-precision dot, as in this tutorial, it must be\n    # in col-major layout.\n    b_ref = MXFP4Tensor(size=(N, K), device=device).random()\n    if block_scale_type in [\"mxfp8\", \"mixed\"]:\n        a_ref = a_ref.to(torch.float32)\n        a = a_ref.to(torch.float8_e4m3fn)\n    else:\n        # Pack two fp4 elements per byte along K\n        a = a_ref.to_packed_tensor(dim=1)\n\n    if block_scale_type == \"mxfp8\":\n        b_ref = b_ref.to(torch.float32)\n        b = b_ref.to(torch.float8_e4m3fn)\n    else:\n        b = b_ref.to_packed_tensor(dim=1)\n\n    b_ref = b_ref.to(torch.float32).T\n\n    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_M, BLOCK_K // ELEM_PER_BYTE_A])\n    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_N, BLOCK_K // ELEM_PER_BYTE_B])\n\n    a_scale_shape = [M // 128, K // VEC_SIZE // 4, 32, 16]\n    b_scale_shape = [N // 128, K // VEC_SIZE // 4, 32, 16]\n    epsilon = 1e-8\n    a_scale = torch.rand(a_scale_shape, device=device) + epsilon\n    b_scale = torch.rand(b_scale_shape, device=device) + epsilon\n    if block_scale_type == \"nvfp4\":\n        a_scale = a_scale.to(torch.float8_e4m3fn)\n        b_scale = b_scale.to(torch.float8_e4m3fn)\n        a_scale_ref = a_scale\n        b_scale_ref = b_scale\n    elif block_scale_type in [\"mxfp4\", \"mxfp8\", \"mixed\"]:\n        a_scale_ref = MXScaleTensor(a_scale)\n        b_scale_ref = MXScaleTensor(b_scale)\n        a_scale = a_scale_ref.data\n        b_scale = b_scale_ref.data\n\n    rep_m = BLOCK_M // 128\n    rep_n = BLOCK_N // 128\n    rep_k = BLOCK_K // VEC_SIZE // 4\n\n    # Use 5D TMA descriptor [1, rep_m, rep_k, 2, 256] with uint8 elements.\n    # With 256 elements we better utilize the L2 and don't require the TMA\n    # engine to emit many small messages (16B) messages as with 32x16xu8.\n    a_scale_block_shape = [1, rep_m, rep_k, 2, 256]\n    b_scale_block_shape = [1, rep_n, rep_k, 2, 256]\n    a_scale = a_scale.reshape(1, a_scale_shape[0], a_scale.shape[1], 2, 256)\n    b_scale = b_scale.reshape(1, b_scale_shape[0], b_scale.shape[1], 2, 256)\n    a_scale_desc = TensorDescriptor.from_tensor(a_scale, block_shape=a_scale_block_shape)\n    b_scale_desc = TensorDescriptor.from_tensor(b_scale, block_shape=b_scale_block_shape)\n\n    reference = None\n    if compute_reference:\n        a_scale_ref = a_scale_ref.to(torch.float32)\n        b_scale_ref = b_scale_ref.to(torch.float32)\n\n        def unpack_scale(packed):\n            packed = packed.reshape(*packed.shape[:-2], 32, 4, 4)\n            num_chunk_m, num_chunk_k, _, _, _ = packed.shape\n            return packed.permute(0, 3, 2, 1, 4).reshape(num_chunk_m * 128, num_chunk_k * 4).contiguous()\n\n        a_scale_ref = unpack_scale(a_scale_ref).repeat_interleave(VEC_SIZE, dim=1)[:M, :K]\n        b_scale_ref = unpack_scale(b_scale_ref).repeat_interleave(VEC_SIZE, dim=1).T.contiguous()[:K, :N]\n        reference = torch.matmul(a_ref.to(torch.float32) * a_scale_ref, b_ref * b_scale_ref)\n\n    configs = {\n        \"BLOCK_SIZE_M\": BLOCK_M,\n        \"BLOCK_SIZE_N\": BLOCK_N,\n        \"BLOCK_SIZE_K\": BLOCK_K,\n        \"num_stages\": 4,\n        \"ELEM_PER_BYTE_A\": ELEM_PER_BYTE_A,\n        \"ELEM_PER_BYTE_B\": ELEM_PER_BYTE_B,\n        \"VEC_SIZE\": VEC_SIZE,\n    }\n    return a_desc, a_scale_desc, b_desc, b_scale_desc, rep_m, rep_n, rep_k, configs, reference\n\n\ndef validate_block_scaled(M, N, K, block_scale_type=\"nvfp4\"):\n    a_desc, a_scale, b_desc, b_scale, rep_m, rep_n, rep_k, configs, reference = initialize_block_scaled(\n        M, N, K, block_scale_type, compute_reference=True)\n    output = block_scaled_matmul(a_desc, a_scale, b_desc, b_scale, torch.float16, M, N, K, rep_m, rep_n, rep_k, configs)\n    torch.testing.assert_close(reference, output.to(torch.float32), atol=1e-3, rtol=1e-3)\n    print(f\"\u2705 (pass {block_scale_type})\")\n\n\ndef bench_block_scaled(K, block_scale_type=\"nvfp4\", reps=10):\n    assert K % 128 == 0\n    M = 8192\n    N = 8192\n    print(f\"Problem Shape = {M}x{N}x{K}\")\n\n    a_desc, a_scale, b_desc, b_scale, rep_m, rep_n, rep_k, configs, _ = initialize_block_scaled(\n        M, N, K, block_scale_type, compute_reference=False)\n    _ = block_scaled_matmul(a_desc, a_scale, b_desc, b_scale, torch.float16, M, N, K, rep_m, rep_n, rep_k, configs)\n\n    proton.activate(0)\n    for _ in range(reps):\n        _ = block_scaled_matmul(a_desc, a_scale, b_desc, b_scale, torch.float16, M, N, K, rep_m, rep_n, rep_k, configs)\n    proton.deactivate(0)\n    print(\"Done benchmarking\")\n\n\ndef show_profile(profile_name):\n    import triton.profiler.viewer as proton_viewer\n\n    metric_names = [\"time/ms\"]\n    metric_names = [\"tflop/s\"] + metric_names\n    file_name = f\"{profile_name}.hatchet\"\n    tree, metrics = proton_viewer.parse(metric_names, file_name)\n    proton_viewer.print_tree(tree, metrics)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-K\", type=int, required=False, default=512)\n    parser.add_argument(\"--K_range\", type=int, nargs=2)\n    parser.add_argument(\"--K_step\", type=int, default=512)\n    parser.add_argument(\"--bench\", action=\"store_true\", default=True)\n    parser.add_argument(\"--format\", type=str, choices=[\"mxfp4\", \"nvfp4\", \"mxfp8\", \"mixed\"], default=\"nvfp4\")\n    args = parser.parse_args()\n\n    if not supports_block_scaling():\n        print(\"\u26d4 This example requires GPU support for block scaled matmul\")\n    else:\n        if args.K and args.K_range is None:\n            args.K_range = [args.K, args.K]\n            args.K_step = 1  # doesn't matter as long as it's not 0\n\n        torch.manual_seed(42)\n\n        validate_block_scaled(8192, 8192, 8192, block_scale_type=args.format)\n\n        if args.bench:\n            proton.start(\"block_scaled_matmul\", hook=\"triton\")\n            proton.deactivate(0)  # Skip argument creation\n            for K in range(args.K_range[0], args.K_range[1] + 1, args.K_step):\n                bench_block_scaled(K, reps=10000, block_scale_type=args.format)\n            proton.finalize()\n            show_profile(\"block_scaled_matmul\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}